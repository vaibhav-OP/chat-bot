{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNi7EINjVQhX"
   },
   "source": [
    "# **Tokenize, Lemmatize, Bag of Word**\n",
    "\n",
    "Tokenize: `lower + lemmatize -> exclude punctuation characters -> bag of word`\n",
    "\n",
    "Example:\n",
    "`\"how are you?\" -> [\"how\", \"are\", \"you\", \"?\"] -> [\"how\", \"are\", \"you\"] -> [0, 0, 1, 1, 0, 1]`\n",
    "\n",
    "using ***NLTK*** to tokenize and lemmatize the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmoDKj1cVQFS",
    "outputId": "8fa0a2b2-c1c3-429d-c988-dfee3f4d8830"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt') # downloads the punkt_tab for tokenize\n",
    "nltk.download('wordnet') # downloads the wordnet for lemmatize\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer as wnl\n",
    "\n",
    "def tokenize(sentence: str):\n",
    "  return nltk.word_tokenize(sentence)\n",
    "\n",
    "def lemmatize(word: str):\n",
    "  return wnl().lemmatize(word.lower())\n",
    "\n",
    "def bag_of_word(sentence: list[str], all_words: list[str]):\n",
    "  '''\n",
    "  return a List of booleans of size all_words\n",
    "  1 if word of sentence existsing in all_words\n",
    "  possition of the 1 will depend on the index of the word in all_words\n",
    "\n",
    "  Example\n",
    "      sentent = [\"how\", \"are\", \"you\"]\n",
    "      all_words = [\"hello\", \"world\", \"how\", \"are\", \"my\", \"friend\", \"you\"]\n",
    "      bag =       [0,        0,       1,     1,     0,    0,        1 ]\n",
    "  '''\n",
    "  lemmatized_sentence = [lemmatize(w) for w in sentence] # Lemmatize the input sentence\n",
    "  bag = np.zeros(len(all_words), dtype=np.float32) # creating a numpy of length all_words\n",
    "\n",
    "  '''\n",
    "    enumarat example\n",
    "\n",
    "    [\"hey\", \"there\"] -> [(0, \"hey\"), (1, \"there\")]\n",
    "  '''\n",
    "\n",
    "  for index, word in enumerate(all_words): # Iterate through all_words\n",
    "    if word in lemmatized_sentence: # Check if the word from all_words is in the lemmatized input sentence\n",
    "      bag[index] = 1.0\n",
    "\n",
    "  return bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbpaJ64TmY0r"
   },
   "source": [
    "# **Neural Network Model**\n",
    "\n",
    "Feed forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VXrQasMrndoL"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "  '''\n",
    "    input_size: size of the input\n",
    "    hidden_size: size of the output of the layer\n",
    "    num_classes: number of classes\n",
    "  '''\n",
    "  def __init__(self, input_size, hidden_size, num_classes):\n",
    "    super(NeuralNetwork, self).__init__()\n",
    "    self.l1 = nn.Linear(input_size, hidden_size)\n",
    "    self.l2 = nn.Linear(hidden_size, hidden_size)\n",
    "    self.l3 = nn.Linear(hidden_size, num_classes)\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.l1(x)\n",
    "    out = self.relu(out)\n",
    "\n",
    "    out = self.l2(out)\n",
    "    out = self.relu(out)\n",
    "\n",
    "    out = self.l3(out)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxHvBdesOFqj"
   },
   "source": [
    "# **Trainer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2dOqjg1XPiEe"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "with open(\"intents.json\", \"r\") as data:\n",
    "  intents = json.load(data)\n",
    "\n",
    "all_words = []\n",
    "tags = []\n",
    "xy = []\n",
    "\n",
    "for intent in intents['intents']:\n",
    "  tag = intent['tag']\n",
    "  tags.append(tag)\n",
    "\n",
    "  for patten in intent['patterns']:\n",
    "    words = tokenize(patten)\n",
    "    all_words.extend(words)\n",
    "    xy.append((words, tag))\n",
    "\n",
    "ignore_words = [\"?\", \".\", \",\", \">\", \"<\", \"!\", \":\", \";\", \"'\", '\"', \"\\\\\", \"/\", \"{\", \"}\", \"[\", \"]\"]\n",
    "all_words = [lemmatize(w) for w in all_words if w not in ignore_words]\n",
    "all_words = sorted(set(all_words)) # Sorts the list and removes any duplicate words using SET properties.\n",
    "tags = sorted(set(tags)) # Sorts and removes duplicate words.\n",
    "\n",
    "X_train = [] # contains the bag of words ex [0, 1, 1, 0, 0, 0, 1]\n",
    "Y_train = [] # contains the index of tags\n",
    "\n",
    "for (words, tag) in xy:\n",
    "  bag = bag_of_word(words, all_words) # Pass all_words to bag_of_word\n",
    "  X_train.append(bag) # Append the bag as a list\n",
    "\n",
    "  label = tags.index(tag)\n",
    "  Y_train.append(label) # Append the label\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duD62w6Dkcfc"
   },
   "source": [
    "# **Data Set**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WAKrma77kfVb",
    "outputId": "38221b7f-e155-4911-b9d5-8de70c959b6b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ChatDataSet(Dataset):\n",
    "  def __init__(self):\n",
    "    self.n_samples = len(X_train) # Number of samples\n",
    "    self.x_data = X_train\n",
    "    self.y_data = Y_train\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.x_data[index], self.y_data[index]\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.n_samples\n",
    "\n",
    "batch_size = 8\n",
    "hidden_size = 16 # Increased hidden size\n",
    "output_size = len(tags)\n",
    "input_size = len(all_words)\n",
    "learning_rate = 0.0005 # Adjusted learning rate\n",
    "num_epochs = 2000\n",
    "\n",
    "data_set = ChatDataSet()\n",
    "train_loader = DataLoader(dataset=data_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = NeuralNetwork(input_size, hidden_size, output_size).to(device) # Move model to GPU\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  for (words, labels) in train_loader:\n",
    "    words = words.to(device) # Move data to GPU\n",
    "    labels = labels.to(device) # Move labels to GPU\n",
    "\n",
    "    # Forwards\n",
    "    outputs = model(words)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    #Backward and optimizer step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0: # Corrected the print condition to print every 100 epochs\n",
    "      print(f\"epoch {epoch+1}/{num_epochs}, loss={loss.item():.4f}\")\n",
    "\n",
    "print(f\"Final loss={loss.item():.4f}\")\n",
    "\n",
    "data = {\n",
    "    'model_state': model.state_dict(),\n",
    "    'input_size': input_size,\n",
    "    'output_size': output_size,\n",
    "    'hidden_size': hidden_size,\n",
    "    'all_words': all_words,\n",
    "    'tags': tags\n",
    "}\n",
    "\n",
    "FILE = \"intents_data.pth\" # Renamed the file\n",
    "torch.save(data, FILE)\n",
    "\n",
    "print(f\"Training complete. File saved to {FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYzBns-1U11W"
   },
   "source": [
    "# **Chat**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cC6u4HsMST62",
    "outputId": "679a510a-2bf1-4cc5-cb8e-65e40654b8cc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import json # Import json to load intents\n",
    "\n",
    "FILE = \"intents_data.pth\" # Updated file name\n",
    "data = torch.load(FILE)\n",
    "\n",
    "tags = data[\"tags\"]\n",
    "all_words = data[\"all_words\"]\n",
    "input_size = data[\"input_size\"]\n",
    "model_state = data[\"model_state\"]\n",
    "hidden_size = data[\"hidden_size\"]\n",
    "output_size = data[\"output_size\"]\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = NeuralNetwork(input_size, hidden_size, output_size).to(device) # Move model to GPU\n",
    "model.load_state_dict(model_state)\n",
    "model.eval() # Sets of eval mode\n",
    "\n",
    "# Load intents data for responses\n",
    "with open(\"intents.json\", \"r\") as json_data:\n",
    "    intents = json.load(json_data)\n",
    "\n",
    "print(\"Press 'quit' to exit\")\n",
    "while True:\n",
    "  sentence = input(\"\")\n",
    "  if sentence == \"quit\":\n",
    "    break\n",
    "\n",
    "  sentence = tokenize(sentence)\n",
    "  X = bag_of_word(sentence, all_words)\n",
    "  X = X.reshape(1, X.shape[0])\n",
    "  X = torch.from_numpy(X).to(device) # Move input tensor to GPU\n",
    "\n",
    "  output = model(X)\n",
    "\n",
    "  _, predicted = torch.max(output, dim=1)\n",
    "  tag = tags[predicted.item()]\n",
    "\n",
    "  probs = torch.softmax(output, dim=1)\n",
    "  prob = probs[0][predicted.item()]\n",
    "\n",
    "  print(prob)\n",
    "\n",
    "  if prob.item() > 0.75:\n",
    "    for intent in intents[\"intents\"]:\n",
    "      if tag == intent[\"tag\"]:\n",
    "        print(random.choice(intent['responses']))\n",
    "  else:\n",
    "    print(\"I can not understand...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VFw3I7b6xAVX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
